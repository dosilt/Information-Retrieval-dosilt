{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e139911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d16a7e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_corpus = json.load(open('dataset/paragraph_context.json', 'r', encoding='utf8')) \n",
    "question_corpus = json.load(open('dataset/question_context.json', 'r', encoding='utf8'))\n",
    "train_labels_json = json.load(open('dataset/train_labels.json', 'r', encoding='utf8'))\n",
    "test_labels_json = json.load(open('dataset/test_labels.json', 'r', encoding='utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecae0607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from transformers import ElectraTokenizer, ElectraModel, ElectraConfig, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e429af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monologg/kobert, \"monologg/koelectra-base-v3-discriminator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b2dea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = list(context_corpus.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cc5e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateDataset(Dataset):\n",
    "    def __init__(self, context_corpus, question_corpus, labels):\n",
    "        self.context_corpus = context_corpus\n",
    "        self.question_corpus = question_corpus\n",
    "        self.labels = self.create_labels(labels)\n",
    "    \n",
    "    def create_labels(self, labels):\n",
    "        new_labels = []\n",
    "        for key in labels.keys():\n",
    "            for t in labels[key]:\n",
    "                new_labels.append([key, t])\n",
    "        print(new_labels[0])\n",
    "        return new_labels\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        doc_id, que_id = self.labels[idx]\n",
    "        return {'context': self.context_corpus[doc_id], 'question': self.question_corpus[que_id]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15f9cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        config = ElectraConfig.from_pretrained(args.model_name, local_file_only=True)\n",
    "        self.model  = ElectraModel.from_pretrained(args.model_name, config=config)\n",
    "        self.tokenizer = ElectraTokenizer.from_pretrained(args.model_name, local_file_only=True)\n",
    "        \n",
    "        self.punctation_idx = self.tokenizer.get_vocab()['.']\n",
    "        self.pad_token_idx = self.tokenizer.pad_token_id\n",
    "        self.mask_token_idx = self.tokenizer.mask_token_id\n",
    "        self.d = self.tokenizer.get_vocab()['[unused0]']\n",
    "        self.q = self.tokenizer.get_vocab()['[unused1]']\n",
    "        self.linear = nn.Linear(config.hidden_size, 256)\n",
    "        \n",
    "        self.doc_maxlen = args.doc_maxlen\n",
    "        self.query_maxlen = args.query_maxlen\n",
    "        self.device = args.device\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        \n",
    "    def forward(self, feature):\n",
    "        q_output = self.query(feature['question'])\n",
    "        d_output = self.doc(feature['context'])\n",
    "        prediction = self.similarity(q_output, d_output)\n",
    "        loss = self.calc_loss(prediction)\n",
    "        return loss\n",
    "    \n",
    "    def calc_loss(self, prediction):\n",
    "        batch_size = prediction.shape[0]\n",
    "        label = torch.arange(batch_size).to(self.device)\n",
    "        return self.criterion(prediction, label)\n",
    "        \n",
    "    \n",
    "    def similarity(self, q_output, d_output):\n",
    "        # q_output = [batch, 128, 256]\n",
    "        # d_output = [batch, seq_len, 256]\n",
    "        prediction = torch.einsum('ijk,abk->iajb', q_output, d_output)\n",
    "        prediction, _ = torch.max(prediction, dim=-1)\n",
    "        prediction = torch.sum(prediction, dim=-1)\n",
    "        return prediction\n",
    "    \n",
    "    \n",
    "    def doc(self, D):\n",
    "        inputs = self.tokenizer(D, return_tensors='pt', padding=True, truncation=True, max_length=self.doc_maxlen)\n",
    "        \n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        \n",
    "        batch = input_ids.shape[0]\n",
    "        \n",
    "        b = torch.LongTensor([self.d]* batch).view(-1, 1)\n",
    "        a = torch.ones(size=(batch, 1))\n",
    "        \n",
    "        input_ids = torch.cat([input_ids[:, :1], b, input_ids[:, 1:]], dim=1).to(self.device)\n",
    "        attention_mask = torch.cat([attention_mask[:, :1], a, attention_mask[:, 1:]], dim=1).to(self.device)\n",
    "        \n",
    "        punctation = input_ids\n",
    "        \n",
    "        model_input = {'input_ids': input_ids,\n",
    "                      'attention_mask': attention_mask}\n",
    "        \n",
    "        output = self.model(**model_input)['last_hidden_state']\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        new_mask = attention_mask * (punctation != self.punctation_idx)\n",
    "        output = output * new_mask.unsqueeze(-1)\n",
    "        output = torch.nn.functional.normalize(output, p=2, dim=2)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def query(self, Q):\n",
    "        inputs = self.tokenizer(Q, return_tensors='pt', truncation=True, max_length=self.query_maxlen,\n",
    "                               pad_to_max_length=True)\n",
    "        \n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        \n",
    "        input_ids = input_ids.masked_fill(attention_mask==self.pad_token_idx, self.mask_token_idx)\n",
    "        \n",
    "        batch = input_ids.shape[0]\n",
    "        \n",
    "        b = torch.LongTensor([self.d]* batch).view(-1, 1)\n",
    "        a = torch.zeros(size=(batch, 1))\n",
    "        \n",
    "        input_ids = torch.cat([input_ids[:, :1], b, input_ids[:, 1:]], dim=1).to(self.device)\n",
    "        new_mask = torch.ones_like(input_ids).to(self.device)\n",
    "        \n",
    "        punctation = input_ids\n",
    "        \n",
    "        model_input = {'input_ids': input_ids,\n",
    "                      'attention_mask': new_mask}\n",
    "        \n",
    "        output = self.model(**model_input)['last_hidden_state']\n",
    "        output = self.linear(output)\n",
    "        output = torch.nn.functional.normalize(output, p=2, dim=2)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a10c291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "from tqdm import tqdm\n",
    "args = easydict.EasyDict({\n",
    "    'model_name': 'monologg/koelectra-base-v3-discriminator',\n",
    "    'doc_maxlen': 512-1,\n",
    "    'query_maxlen': 128-1,\n",
    "    'device': 'cuda',\n",
    "    'epochs': 5,\n",
    "    'warmup': 0.1,\n",
    "    'batch_size': 16\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f55ff1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Model(args).to(args.device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ebb1afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PARS_1agoYxToKo', 'QUES_TNC71lb33r']\n"
     ]
    }
   ],
   "source": [
    "dataset = CreateDataset(context_corpus, question_corpus, train_labels_json)\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cab9f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "max_step = len(dataloader) * args.epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup * max_step, num_training_steps=max_step)\n",
    "pre_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fc8322b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                        | 0/11652 [00:00<?, ?it/s]C:\\Users\\Inha\\anaconda3\\envs\\inha\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2212: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 11652/11652 [57:17<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 loss= 0.279783604182856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 11652/11652 [58:56<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 1 loss= 0.03537065577154934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 11652/11652 [57:51<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 2 loss= 0.01118345380851512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 11652/11652 [56:20<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 3 loss= 0.0043108137073284035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 11652/11652 [56:43<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 4 loss= 0.0017395397259050965\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    model.train()\n",
    "    avg_loss = []\n",
    "    for x in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(x)\n",
    "        loss.backward()\n",
    "        avg_loss.append(loss.item())\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    print('epoch=', epoch, 'loss=', np.mean(avg_loss))\n",
    "    \n",
    "    if np.mean(avg_loss) < pre_loss:\n",
    "        torch.save(model, 'best.pt')\n",
    "torch.save(model, 'last.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c219a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
